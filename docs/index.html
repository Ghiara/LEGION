<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Preserving and Combining Knowledge in Robotic Lifelong Reinforcement Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/tum_icon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Preserving and Combining Knowledge in Robotic Lifelong Reinforcement Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Yuan Meng</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a target="_blank">Zhenshan Bing</a><sup>1,2*&dagger;</sup>,</span>
                  <span class="author-block">
                    <a target="_blank">Xiangtong Yao</a><sup>1*</sup>,</span>
                    <span class="author-block">
                      <a target="_blank">Kejia Chen</a><sup>1</sup>,</span>
                      <br>
                      <span class="author-block">
                        <a target="_blank">Kai Huang</a><sup>3&dagger;</sup>,</span>
                        <span class="author-block">
                          <a target="_blank">Yang Gao</a><sup>2&dagger;</sup>,</span>
                          <span class="author-block">
                            <a target="_blank">Fuchun Sun</a><sup>4&dagger;</sup>,</span>
                            <span class="author-block">
                              <a target="_blank">Alois Knoll</a><sup>1</sup>,
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <small><sup>1</sup>School of Computation, Information and Technology, Technical University of Munich, Germany</small>
                      <br><small><sup>2</sup>State Key Laboratory for Novel Software Technology, Nanjing University, China</small>
                      <br><small><sup>3</sup>Key Laboratory of Machine Intelligence and Advanced Computing, School of Computer Science and Engineering, Sun Yat-sen University, China</small>
                      <br><small><sup>4</sup>Department of Computer Science and Technology, Tsinghua University, China</small>
                    </span>
                    <span class="eql-cntrb"><br><sup>*</sup>Indicates Equal Contribution</span>
                    <span class="corresponding"><small><br><sup>&dagger;</sup>To whom correspondence should be addressed; E-mail: zhenshan.bing@tum.de, huangk36@mail.sysu.edu.cn, gaoy@nju.edu.cn, fcsun@tsinghua.edu.cn</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://www.nature.com/articles/s42256-025-00983-2" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://www.nature.com/articles/s42256-025-00983-2" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Ghiara/LEGION" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video 1-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
        <source src="static/videos/demo1.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-justified">
        <b>Simulation and real-world demonstrations of acquired skills during lifelong reinforcemnet learning process</b>. 
        In this study, the agent is sequentially trained on multiple tasks, adhering to a lifelong reinforcement learning process. 
        Our proposed framework employs a Bayesian non-parametric model to manage its prior knowledge space, allowing it to cluster 
        and retain knowledge from the continuous stream of tasks, which significantly enhances overall task success. 
        The video showcases both simulation and real-world demonstrations, illustrating that our framework successfully 
        completes all tasks encountered during the lifelong learning process.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video 1-->




<!-- Teaser video 2-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
        <source src="https://assets-eu.researchsquare.com/files/rs-4353532/v1/49b2a9646c62385bb89ddc0e.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-justified">
        <b>Demonstration of solving the long-horizon task ``clean the table''</b>. 
        Our agent successfully completes the long-horizon task by combining knowledge acquired from the previous lifelong learning process. 
        Unlike recent studies that rely on human demonstrations, our agent demonstrates greater flexibility by learning each subtask independently, 
        without the need for strict conditioning between tasks. This highlights its ability to generalize and adapt to more challenging 
        non-parametric task distributions.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video 2-->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Humans can continually accumulate knowledge and develop increasingly complex behaviors and skills throughout their lives, which is a capability known as ``lifelong learning''. 
            Although this lifelong learning capability is considered an essential mechanism that makes up generalized intelligence, recent advancements in artificial intelligence predominantly excel in narrow, specialized domains and generally lack of this lifelong learning capability.
            Our study introduces a robotic lifelong reinforcement learning framework that addresses this gap by incorporating a non-parametric Bayesian model into the knowledge space.
            Additionally, we enhance the agent's semantic understanding of tasks by integrating language embeddings into the framework.
            Our proposed embodied agent can consistently accumulate knowledge from a continuous stream of one-time feeding tasks. 
            Furthermore, our agent can tackle challenging real-world long-horizon tasks by combining and reapplying its acquired knowledge from the original tasks stream.
            Our findings demonstrate that intelligent embodied agents can exhibit a capability for lifelong learning similar to that of human beings.
            The proposed framework advances our understanding of the robotic lifelong learning process and may inspire the development of more broadly applicable intelligence.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/img1.png" alt="general LRL overview" class="center"/>
        <h2 class="subtitle has-text-justified">
          <b>Concept illustration of robotic lifelong reinforcement learning process</b>. 
          (<b>A</b>) Overview illustration of the general lifelong reinforcement learning process. 
          Unlike the conventional multi-task approaches, where agents have simultaneous access to all tasks, 
          a LRL agent can master tasks sequentially one after another. Moreover, the agent should continually 
          accumulate knowledge throughout the process. This concept emulates the human learning process. 
          (<b>B</b>) Our proposed framework under lifelong learning concept. 
          We instruct the deployed embodied agent to perform long-horizon tasks using language commands. 
          The agent accomplishes these tasks through the combination and reapplication of acquired knowledge.
        </h2>
      </div>

      <div class="item">
        <!-- Your image here -->
        <img src="static/images/framework_train.png" alt="Training" class="center"/>
        <h2 class="subtitle has-text-justified">
          (<b>A</b>) Training: The framework receives language semantic information and environment observations 
          as input to make policy decisions and output action patterns, it trains on only one task at a time. L
          represents the loss functions and is explained in the Method section ``Upstream task inference''.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/framework_deployment.png" alt="Deployment" class="center"/>
        <h2 class="subtitle has-text-justified">
          (<b>B</b>) Deployment: In the real-world demonstration, the agent parameters remain frozen, the agent 
          receives input signal from real-world hardware and outputs corresponding action signals, both ``sim2real'' and ``real2sim'' 
          modules process the data to align the gap between the simulation and real world.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/img2.png" alt="Success rate" class="center"/>
      <h2 class="subtitle has-text-justified">
        <b>Contribution of non-parametric knowledge space w/o language embeddings</b>. (<b>A</b>) Average success rate comparison. 
        Comparing LEGION (Gauss) and LEGION (DPMM) provides insights into the contribution of the non-parametric knowledge space. 
        Similarly, comparing SAC (w. Language embeddings) and SAC (w.o. Language embeddings) allows us to assess the contribution of language embeddings.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/img3.png" alt="t-SNE projection" class="center"/>
      <h2 class="subtitle has-text-justified">
        <b>T-SNE snapshots of knowledge space</b>. (<b>A</b>)-(<b>E</b>) T-SNE projection of latent space after training on two tasks, 
        four tasks, six tasks, eight tasks and all tasks, respectively.
      </h2>
    </div>

    
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
            <!-- <iframe src="https://assets-eu.researchsquare.com/files/rs-4353532/v1/d8c2289d1ccbba09ee8c9558.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Supplementary Videos</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="https://assets-eu.researchsquare.com/files/rs-4353532/v1/d8c2289d1ccbba09ee8c9558.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="https://assets-eu.researchsquare.com/files/rs-4353532/v1/af868845c09ee8787c349fdd.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="https://assets-eu.researchsquare.com/files/rs-4353532/v1/26b86b788635730e962b8a99.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/demo_coffee-v2.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper (Preprint)</h2>

      <iframe  src="https://assets-eu.researchsquare.com/files/rs-4353532/v1_covered_322b1292-bd58-4238-acd6-5222f1794da2.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>
        Meng, Y., Bing, Z., Yao, X. et al. Preserving and combining knowledge in robotic lifelong reinforcement learning. Nat Mach Intell (2025). https://doi.org/10.1038/s42256-025-00983-2
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
